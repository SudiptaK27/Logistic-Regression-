{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Logistic Regression is a statistical and machine learning technique used for solving classification problems, particularly when the dependent variable is categorical in nature. The most common form is binary classification, where the output can take only two possible values such as yes/no, spam/not spam, disease/no disease, or 0/1. Logistic Regression does not directly predict the class; instead, it predicts the probability of the input belonging to a particular class.\n",
        "To achieve this, it uses the logistic (sigmoid) function, which transforms any real-valued number into a value between 0 and 1. Based on a threshold (usually 0.5), the final class is decided. The model estimates the relationship between the independent variables and the log-odds of the dependent variable, making it suitable for classification tasks where linear relationships do not naturally fit into probability constraints.\n",
        "\n",
        "Linear Regression, in contrast, is a method used for predicting continuous numerical outcomes. It assumes a linear relationship between the dependent variable and one or more independent variables. The model produces a straight-line equation of the form:\n",
        "Y = β₀ + β₁X₁ + β₂X₂ + … + ε,\n",
        "where Y is the predicted value. Linear Regression outputs numbers that can take any value from negative infinity to positive infinity, which is not suitable for classification or probability estimation.\n",
        "\n",
        "Major Differences Between Logistic and Linear Regression\n",
        "\n",
        "**Type of Problem Solved**\n",
        "• Logistic Regression is used for classification problems (binary or multi-class).\n",
        "• Linear Regression is used for regression problems involving continuous values.\n",
        "\n",
        "**Nature of Output**\n",
        "• Logistic Regression outputs probabilities between 0 and 1 and later converts them into classes.\n",
        "• Linear Regression outputs continuous numeric values without restrictions.\n",
        "\n",
        "**Mathematical Function Used**\n",
        "• Logistic Regression uses the sigmoid function to map the output into a probability range.\n",
        "• Linear Regression uses a linear function, forming a straight line in the feature space.\n",
        "\n",
        "**Interpretation of Parameters**\n",
        "• In Logistic Regression, coefficients represent odds ratios, explaining how the log-odds change with independent variables.\n",
        "• In Linear Regression, coefficients indicate how much the dependent variable changes with a unit change in an independent variable.\n",
        "\n",
        "**Error Minimization Technique**\n",
        "• Logistic Regression uses Maximum Likelihood Estimation (MLE) to find the best parameters.\n",
        "• Linear Regression uses Ordinary Least Squares (OLS) to minimize the sum of squared errors.\n",
        "\n",
        "**Range of Predictions**\n",
        "• Logistic Regression predictions lie strictly between 0 and 1.\n",
        "• Linear Regression predictions may lie anywhere on the real number line.\n",
        "\n",
        "**Assumptions**\n",
        "• Logistic Regression does not assume linearity between independent variables and the output; instead, it assumes linearity with the logit.\n",
        "• Linear Regression assumes a direct linear relationship between variables, homoscedasticity, and normally distributed errors.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "In summary, Logistic Regression and Linear Regression are both foundational statistical models, but they serve very different purposes. Logistic Regression is appropriate when the goal is to classify data into categories and interpret probabilities, while Linear Regression is suitable for predicting continuous numeric outcomes and understanding linear relationships. Their mathematical functions, output ranges, assumptions, and evaluation techniques differ significantly, making each model ideal for specific types of real-world problems.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Question 2: Explain the role of the Sigmoid function in Logistic Regression.**\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "The Sigmoid function, also known as the logistic function, plays a central and essential role in Logistic Regression. Logistic Regression is used for classification, especially in binary classification problems where the output variable can take only two values such as 0 or 1, yes or no, or true or false. Since the goal is to estimate the probability of an event occurring, the model needs a mathematical function that can convert any real-valued number into a value strictly between 0 and 1. The Sigmoid function provides exactly this capability, making it the foundation of Logistic Regression.\n",
        "\n",
        "**The Sigmoid function is defined in single-line form as:**\n",
        "\n",
        "σ(z) = 1 / (1 + e^(–z))\n",
        "\n",
        "Here, z represents the linear combination of input features\n",
        "z = β₀ + β₁X₁ + β₂X₂ + …\n",
        "The Sigmoid function converts this output into a probability-like value between 0 and 1.\n",
        "\n",
        "Key Roles of the Sigmoid Function in Logistic Regression\n",
        "1. Converts Linear Output into Probability\n",
        "\n",
        "The primary role of the Sigmoid function is to map the linear output of the model into a probability range between 0 and 1. Without this transformation, predictions could become negative or exceed 1, which cannot represent probabilities.\n",
        "\n",
        "2. Enables Classification\n",
        "\n",
        "After obtaining the probability from the Sigmoid function, a threshold (usually 0.5) is applied:\n",
        "• If probability ≥ 0.5 → Class 1\n",
        "• If probability < 0.5 → Class 0\n",
        "This classification step is possible only because the Sigmoid output is always between 0 and 1.\n",
        "\n",
        "3. Introduces Non-Linearity\n",
        "\n",
        "Even though Logistic Regression uses a linear equation inside, the Sigmoid function introduces non-linearity, allowing the model to capture more complex relationships between features and the output.\n",
        "\n",
        "4. Smooth and Differentiable Curve\n",
        "\n",
        "The Sigmoid curve is smooth and differentiable, which is essential for optimization. Logistic Regression uses Maximum Likelihood Estimation (MLE) and gradient-based methods like Gradient Descent. The Sigmoid function’s smooth gradient makes training efficient and stable.\n",
        "\n",
        "5. Interpretable Output\n",
        "\n",
        "The Sigmoid output can be directly interpreted as the probability of belonging to the positive class. This is useful in real-world applications such as medical diagnosis, credit scoring, fraud detection, and risk prediction.\n",
        "\n",
        "6. Ensures Output Stability\n",
        "\n",
        "When z becomes very large or very small, the Sigmoid function approaches 1 or 0 but never reaches them exactly. This prevents numerical instability and avoids producing extreme or invalid probability values.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "The Sigmoid function is essential in Logistic Regression because it converts linear outputs into meaningful probability scores. This conversion enables classification, supports gradient-based optimization, and ensures stable, interpretable predictions. Without the Sigmoid function, Logistic Regression would not be able to perform its key purpose: estimating the probability of an event and making accurate class predictions.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question 3: What is Regularization in Logistic Regression and why is it needed?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Regularization in Logistic Regression is a technique used to prevent the model from becoming overly complex and overfitting the training data. Overfitting occurs when a model learns the noise, random fluctuations, or useless patterns in the data instead of learning the true underlying relationship. Regularization solves this by adding a penalty term to the cost function of Logistic Regression, which discourages excessively large coefficient values. By controlling the magnitude of the model parameters, regularization encourages the model to remain simpler, more generalizable, and more robust when applied to new, unseen data.\n",
        "\n",
        "In Logistic Regression, the regularized cost function can be written in single-line form as:\n",
        "\n",
        "Cost Function = –log(likelihood) + λ × (penalty term)\n",
        "\n",
        "Here, λ (lambda) is the regularization parameter that determines the strength of the penalty. A higher value of λ increases the penalty, forcing the coefficients to shrink toward zero.\n",
        "\n",
        "Types of Regularization Used in Logistic Regression\n",
        "**L1 Regularization (Lasso)**\n",
        "\n",
        "L1 adds the absolute value of coefficients as a penalty. It can shrink some coefficients exactly to zero, leading to feature selection. This is useful when the dataset contains many irrelevant or weak features.\n",
        "\n",
        "**L2 Regularization (Ridge)**\n",
        "\n",
        "L2 adds the square of coefficient values as a penalty. Instead of eliminating features, it shrinks all coefficients smoothly, reducing their magnitude. L2 is widely used because it stabilizes the model and reduces variance.\n",
        "\n",
        "Why Regularization is Needed\n",
        "**Prevents Overfitting**\n",
        "\n",
        "Without regularization, Logistic Regression may produce very large coefficients while trying to perfectly classify the training data. This makes the model extremely sensitive to noise. Regularization limits coefficient size and ensures the model learns general patterns instead of noise.\n",
        "\n",
        "**Improves Model Generalization**\n",
        "\n",
        "The goal of any model is to perform well on unseen data. By penalizing complexity, regularization ensures better and more stable performance on both training and testing sets.\n",
        "\n",
        "**Reduces Variance**\n",
        "\n",
        "Models with high variance produce inconsistent results depending on the training sample. Regularization smooths the decision boundary, making predictions more stable and reliable.\n",
        "\n",
        "**Handles Multicollinearity**\n",
        "\n",
        "When independent variables are highly correlated, coefficients can become unstable. L2 regularization helps control this by shrinking correlated feature weights and reducing noise amplification.\n",
        "\n",
        "**Improves Interpretation and Simplicity**\n",
        "\n",
        "Large coefficients make the model harder to interpret. Regularization keeps coefficients smaller and meaningful. L1 can even remove unnecessary features, making the model simpler.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Regularization is an essential component of Logistic Regression, ensuring that the model is not only accurate on the training data but also robust, stable, and well-generalized to new data. By adding a penalty for large coefficients, regularization protects the model from overfitting, improves interpretability, and enhances performance in real-world applications. Thus, regularization plays a vital role in making Logistic Regression both practical and reliable for classification tasks.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question 4: What are some common evaluation metrics for classification models, and why are they important?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Evaluating a classification model requires more than just checking how many predictions it gets right. Different problems have different consequences for errors, and some datasets are imbalanced, making simple accuracy unreliable. Therefore, several evaluation metrics are used to understand a model’s performance more thoroughly.\n",
        "\n",
        "1. Accuracy\n",
        "\n",
        "Accuracy measures the proportion of correct predictions out of all predictions.\n",
        "\n",
        "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "Although simple, accuracy becomes misleading in imbalanced datasets because a model can achieve high accuracy by predicting only the majority class while completely ignoring the minority class.\n",
        "\n",
        "2. Precision\n",
        "\n",
        "Precision measures how many of the predicted positive cases were actually positive.\n",
        "\n",
        "Precision = TP / (TP + FP)\n",
        "\n",
        "Precision is crucial in situations where false positives cause harm, such as spam detection, fraud detection, or medical tests that should avoid unnecessary alarms.\n",
        "\n",
        "3. Recall (Sensitivity / True Positive Rate)\n",
        "\n",
        "Recall measures how many actual positive cases the model correctly identifies.\n",
        "\n",
        "Recall = TP / (TP + FN)\n",
        "\n",
        "Recall is important when false negatives are more dangerous—as in disease diagnosis, where missing a positive patient can have severe consequences.\n",
        "\n",
        "4. F1-Score\n",
        "\n",
        "The F1-score is the harmonic mean of precision and recall, giving a balanced view when both false positives and false negatives matter.\n",
        "\n",
        "F1 = 2 × (Precision × Recall) / (Precision + Recall)\n",
        "\n",
        "It is especially useful for imbalanced datasets where precision and recall are both important.\n",
        "\n",
        "5. Confusion Matrix\n",
        "\n",
        "A confusion matrix is a table showing TP, TN, FP, and FN. It provides a complete picture of how the model performs on each class and helps identify whether the model is biased or making specific kinds of mistakes.\n",
        "\n",
        "6. ROC Curve and AUC (Area Under Curve)\n",
        "\n",
        "The ROC curve plots True Positive Rate vs False Positive Rate at different thresholds.\n",
        "AUC summarizes this performance into a single number between 0 and 1.\n",
        "Higher AUC means better class separation.\n",
        "These metrics are useful because they do not depend on one fixed threshold.\n",
        "\n",
        "7. Log Loss (Cross-Entropy Loss)\n",
        "\n",
        "Log Loss evaluates how well the model predicts probabilities, not just class labels.\n",
        "Models that assign poor probabilities get high Log Loss.\n",
        "This is important in applications needing accurate risk estimation (e.g., finance, insurance, healthcare).\n",
        "\n",
        "**Importance of These Metrics**\n",
        "\n",
        "Evaluation metrics are important because they:\n",
        "• Give a deeper view of performance beyond accuracy.\n",
        "• Highlight strengths and weaknesses that accuracy hides.\n",
        "• Match different real-world needs (recall for medical tests, precision for fraud detection).\n",
        "• Help compare models fairly.\n",
        "• Detect issues like imbalance, bias, and overfitting.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Metrics such as Accuracy, Precision, Recall, F1-Score, Confusion Matrix, ROC-AUC, and Log Loss provide a complete understanding of classification model performance. Each metric focuses on different aspects of prediction quality, ensuring the chosen model is accurate, reliable, and appropriate for real-world use.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jICCAF1Nptkz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QXeOZ7slped-",
        "outputId": "049277fc-1f71-4313-fc23-0aca03d21d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ],
      "source": [
        "''' Question 5:  Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "# Save to CSV (simulating a CSV file)\n",
        "csv_filename = \"breast_cancer.csv\"\n",
        "df.to_csv(csv_filename, index=False)\n",
        "\n",
        "# 2. Load the CSV file into a Pandas DataFrame\n",
        "data_df = pd.read_csv(csv_filename)\n",
        "\n",
        "# Separate features and target\n",
        "X = data_df.drop(\"target\", axis=1)\n",
        "y = data_df[\"target\"]\n",
        "\n",
        "# 3. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=5000, solver=\"liblinear\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict on test set and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nuIFNoCmxLnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 6: Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "# 2. Split features and target\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# 3. Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Logistic Regression with L2 Regularization (default = L2)\n",
        "model = LogisticRegression(\n",
        "    penalty=\"l2\",\n",
        "    C=1.0,          # Inverse of regularization strength (lower = stronger regularization)\n",
        "    solver=\"liblinear\",\n",
        "    max_iter=5000\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Print model coefficients and accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Coefficients:\\n\", model.coef_)\n",
        "print(\"\\nModel Intercept:\\n\", model.intercept_)\n",
        "print(\"\\nAccuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VgcwkpSyvHTD",
        "outputId": "26a1cd87-b33b-4dd0-f88d-e121069a579d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            " [[ 2.13248406e+00  1.52771940e-01 -1.45091255e-01 -8.28669349e-04\n",
            "  -1.42636015e-01 -4.15568847e-01 -6.51940282e-01 -3.44456106e-01\n",
            "  -2.07613380e-01 -2.97739324e-02 -5.00338038e-02  1.44298427e+00\n",
            "  -3.03857384e-01 -7.25692126e-02 -1.61591524e-02 -1.90655332e-03\n",
            "  -4.48855442e-02 -3.77188737e-02 -4.17516190e-02  5.61347410e-03\n",
            "   1.23214996e+00 -4.04581097e-01 -3.62091502e-02 -2.70867580e-02\n",
            "  -2.62630530e-01 -1.20898539e+00 -1.61796947e+00 -6.15250835e-01\n",
            "  -7.42763610e-01 -1.16960181e-01]]\n",
            "\n",
            "Model Intercept:\n",
            " [0.40847797]\n",
            "\n",
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XHpgEksMxOAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load a multiclass dataset (Iris dataset has 3 classes)\n",
        "data = load_iris()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "# 2. Split features and target\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Logistic Regression with One-vs-Rest (OvR)\n",
        "model = LogisticRegression(\n",
        "    multi_class='ovr',\n",
        "    solver='liblinear',\n",
        "    max_iter=5000\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions and classification report\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Hb1dyAJrvhzE",
        "outputId": "4e97f347-fae4-473b-b4b9-84ab69f13be6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "# 2. Split features and target\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Hyperparameter grid for Logistic Regression\n",
        "param_grid = {\n",
        "    \"C\": [0.01, 0.1, 1, 10, 100],         # Regularization strength\n",
        "    \"penalty\": [\"l1\", \"l2\"],             # L1 or L2\n",
        "    \"solver\": [\"liblinear\"]              # liblinear supports both L1 and L2\n",
        "}\n",
        "\n",
        "# 5. Apply GridSearchCV\n",
        "grid = GridSearchCV(\n",
        "    LogisticRegression(max_iter=5000),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 6. Best parameters and validation accuracy\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
        "\n",
        "# 7. Test-set accuracy using best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FQb8vNF-vzpm",
        "outputId": "b70befac-0771-467b-e4f5-3a6092fb8d24"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Cross-Validation Accuracy: 0.9670329670329672\n",
            "Test Accuracy: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MNWih0_xxQJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 9: Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling.\n",
        "\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Part 1: Logistic Regression WITHOUT Scaling\n",
        "# ---------------------------------------------------------\n",
        "model_no_scaling = LogisticRegression(max_iter=5000, solver=\"liblinear\")\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Part 2: Logistic Regression WITH Standardization\n",
        "# ---------------------------------------------------------\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit scaler on training data & transform both sets\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=5000, solver=\"liblinear\")\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Print Results\n",
        "# ---------------------------------------------------------\n",
        "print(\"Accuracy WITHOUT Scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy WITH Scaling:\", accuracy_scaled)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0HEEb5pcwULo",
        "outputId": "c784e048-2537-42d8-a90a-bd5ca88dc0cf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Scaling: 0.956140350877193\n",
            "Accuracy WITH Scaling: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XeMAUtlnxSWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business use case.**\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "In this scenario, the e-commerce company wants to predict which customers will respond to a marketing campaign. Since only 5% of customers respond, the dataset is highly imbalanced, and a direct Logistic Regression without proper preprocessing will perform poorly. A carefully designed workflow is required to ensure that the model provides reliable predictions that the business can use.\n",
        "\n",
        "1. Understanding the Data and Preprocessing\n",
        "\n",
        "The dataset likely includes demographic features, browsing history, past purchases, marketing interactions, and engagement signals. The first step is to clean the data:\n",
        "\n",
        "• Handle missing values\n",
        "• Convert categorical variables using one-hot encoding\n",
        "• Remove duplicates and outliers\n",
        "• Identify leakage features (e.g., post-campaign behaviour)\n",
        "\n",
        "Since Logistic Regression is sensitive to differences in scale, continuous features must be standardized before training.\n",
        "\n",
        "2. Feature Scaling\n",
        "\n",
        "Logistic Regression uses distance-based optimization, so variables on different scales (for example, “age” vs “annual income”) can distort the model.\n",
        "\n",
        "I would apply:\n",
        "\n",
        "StandardScaler = (x – mean) / standard deviation\n",
        "\n",
        "This keeps all features at similar magnitude, improves convergence, and stabilizes the coefficient values.\n",
        "\n",
        "3. Handling Class Imbalance (Only 5% Positive Class)\n",
        "\n",
        "A normal model would likely predict every customer as “non-responder” and still get 95% accuracy, which is useless.\n",
        "\n",
        "To solve this, we use class balancing techniques:\n",
        "\n",
        "a. Class Weights\n",
        "\n",
        "Logistic Regression has a built-in option:\n",
        "\n",
        "class_weight = \"balanced\"\n",
        "\n",
        "This increases the penalty for misclassifying positive cases, helping the model pay attention to the minority group.\n",
        "\n",
        "b. Oversampling and Undersampling\n",
        "\n",
        "• SMOTE (Synthetic Minority Oversampling Technique) to generate synthetic minority samples\n",
        "• Random oversampling to increase the number of responders\n",
        "• Undersampling the majority class if dataset is large\n",
        "\n",
        "SMOTE + class_weight usually gives the best performance.\n",
        "\n",
        "4. Hyperparameter Tuning\n",
        "\n",
        "I would apply GridSearchCV to tune parameters such as:\n",
        "\n",
        "• C → Regularization strength\n",
        "• penalty → L1 or L2\n",
        "• class_weight → balanced or custom\n",
        "• solver → liblinear / saga depending on penalty\n",
        "\n",
        "This ensures the model is neither underfitting nor overfitting, while properly handling imbalanced classes.\n",
        "\n",
        "Sample search space:\n",
        "\n",
        "C = [0.01, 0.1, 1, 10]\n",
        "penalty = ['l1', 'l2']\n",
        "class_weight = ['balanced', None]\n",
        "\n",
        "5. Model Evaluation for a Business Use Case\n",
        "\n",
        "Accuracy is misleading in imbalanced problems; therefore, alternative metrics must be used.\n",
        "\n",
        "Important Metrics:\n",
        "\n",
        "• Precision – How many predicted responders actually respond\n",
        "• Recall (Sensitivity) – How many actual responders the model captures\n",
        "• F1-Score – Balance between precision and recall\n",
        "• ROC-AUC – Model’s ability to distinguish responders vs non-responders\n",
        "• Precision-Recall AUC – More meaningful when positives are rare\n",
        "• Confusion Matrix – To understand types of errors\n",
        "\n",
        "For marketing, recall is crucial because missing potential responders means losing revenue. But precision also matters because targeting too many uninterested customers wastes campaign budget.\n",
        "\n",
        "Hence, I would optimize the model for a balance based on business goals.\n",
        "\n",
        "6. Using Business Logic and Threshold Tuning\n",
        "\n",
        "Logistic Regression outputs probabilities. Instead of using the default threshold of 0.5, I would adjust it.\n",
        "\n",
        "For example:\n",
        "\n",
        "• Lower threshold (0.2–0.3) → catch more responders (high recall)\n",
        "• Higher threshold → fewer campaign emails (high precision)\n",
        "\n",
        "This threshold is chosen based on marketing budget, cost per campaign, and expected return.\n",
        "\n",
        "7. Final Deployment Plan\n",
        "\n",
        "Preprocess and scale the data\n",
        "\n",
        "Balance the classes using SMOTE or class_weight\n",
        "\n",
        "Train Logistic Regression with tuned hyperparameters\n",
        "\n",
        "Evaluate using recall, precision, F1, and PR-AUC\n",
        "\n",
        "Choose a threshold aligned with campaign goals\n",
        "\n",
        "Deploy the model and monitor performance continuously\n",
        "\n",
        "Retrain periodically as customer behaviour changes\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "To build a robust Logistic Regression model for an imbalanced marketing campaign dataset, it is essential to carefully preprocess the data, standardize features, handle imbalance using SMOTE or class weights, tune hyperparameters, and evaluate using appropriate metrics like precision, recall, F1-score, and ROC-AUC. With proper threshold tuning and continuous monitoring, the model can provide actionable insights and help the company target the right customers effectively."
      ],
      "metadata": {
        "id": "ORTm5tOSwrs5"
      }
    }
  ]
}